{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"open-existing-db\"></a>\n",
    "###  Import the correct libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//import libraries\n",
    "import org.apache.spark.{SparkConf, SparkContext, SparkFiles}\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession, Row}\n",
    "import org.apache.spark.SparkFiles\n",
    "\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.ibm.transformers.RenameColumn\n",
    "\n",
    "import com.ibm.analytics.ngp.repository._\n",
    "import com.ibm.analytics.ngp.ingest.Sampling\n",
    "import com.ibm.analytics.ngp.util._\n",
    "import com.ibm.analytics.ngp.pipeline.evaluate.{Evaluator,MLProblemType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"open-existing-db\"></a>\n",
    "###  Open the IBM Db2 Event store database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import com.ibm.event.oltp.EventContext\n",
    "import com.ibm.event.common.ConfigurationReader\n",
    "ConfigurationReader.setConnectionEndpoints(\"173.19.0.1:1100\")\n",
    "val eContext = EventContext.getEventContext(\"TESTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"validate-db\"></a>\n",
    "###  Validate that the table have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val raw_weather_data = eContext.getTable(\"raw_weather_data\")\n",
    "val sky_condition_lookup = eContext.getTable(\"sky_condition_lookup\")\n",
    "val monthly_aggregate_precip = eContext.getTable(\"monthly_aggregate_precip\")\n",
    "val monthly_aggregate_windspeed = eContext.getTable(\"monthly_aggregate_windspeed\")\n",
    "val monthly_aggregate_pressure = eContext.getTable(\"monthly_aggregate_pressure\")\n",
    "val monthly_aggregate_temperature = eContext.getTable(\"monthly_aggregate_temperature\")\n",
    "val daily_aggregate_precip = eContext.getTable(\"daily_aggregate_precip\")\n",
    "val daily_aggregate_windspeed = eContext.getTable(\"daily_aggregate_windspeed\")\n",
    "val daily_aggregate_pressure = eContext.getTable(\"daily_aggregate_pressure\")\n",
    "val daily_aggregate_temperature = eContext.getTable(\"daily_aggregate_temperature\")\n",
    "val daily_predicted_temperature = eContext.getTable(\"daily_predicted_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"create-sqlContext\"></a>\n",
    "### Create the IBM Db2 EventSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "import com.ibm.event.oltp.EventContext\n",
    "import org.apache.log4j.{Level, LogManager, Logger}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.ibm.event.EventSession\n",
    "\n",
    "val sqlContext = new EventSession(spark.sparkContext, \"TESTDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"prepare-DataFrame\"></a>\n",
    "### Prepare a DataFrame for the query \n",
    "The following API provides a DataFrame that holds the query results on the IBM Db2 Event Store table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:38: error: value loadEventTable is not a member of org.apache.spark.sql.SQLContext\n",
       "       val dfDailyTemp = sqlContext.loadEventTable(\"daily_aggregate_temperature\")\n",
       "                                    ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfDailyTemp = sqlContext.loadEventTable(\"daily_aggregate_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: key not found: DSX_PROJECT_DIR\n",
       "StackTrace:   at scala.collection.MapLike$class.default(MapLike.scala:228)\n",
       "  at scala.collection.AbstractMap.default(Map.scala:59)\n",
       "  at scala.collection.MapLike$class.apply(MapLike.scala:141)\n",
       "  at scala.collection.AbstractMap.apply(Map.scala:59)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlContext = new SQLContext(sc)\n",
    "// Add data asset from file system\n",
    "val dfDailyTemp = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv(sys.env(\"DSX_PROJECT_DIR\")+\"/datasets/daily_aggregate_temperature.csv\")\n",
    "dfDailyTemp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- wsid: long (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- ts: long (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- mean: double (nullable = false)\n",
      " |-- variance: double (nullable = false)\n",
      " |-- stdev: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDailyTemp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+---+-------------+----+---+------------------+-----------------+------------------+\n",
      "|       wsid|year|month|day|           ts|high|low|              mean|         variance|             stdev|\n",
      "+-----------+----+-----+---+-------------+----+---+------------------+-----------------+------------------+\n",
      "|72494023234|2011|    8|  1|1314860400657|12.2|0.0| 8.054166666666667|9.104982638888883|3.0174463771356206|\n",
      "|72494023234|2011|    8|  2|1314946800663|11.7|0.0|7.9458333333333355|7.534982638888888|2.7449922839397725|\n",
      "|72494023234|2011|    8|  3|1315033200126|14.4|0.0|10.991666666666665|6.780763888888887|2.6039899940070597|\n",
      "|72494023234|2011|    8|  4|1315119600128|15.0|0.0|11.916666666666668|8.223055555555556|2.8675870615476624|\n",
      "|72494023234|2011|    8|  5|1315206000114|12.2|0.0| 9.679166666666665|5.934982638888889| 2.436181979838306|\n",
      "+-----------+----+-----+---+-------------+----+---+------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val weatherStations = dfDailyTemp.select(\"wsid\").distinct.collect.flatMap(_.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._\n",
    "val weatherStationsArray = weatherStations.map(ws => dfDailyTemp.where($\"wsid\" <=> ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Weather Station ID is 72494023234\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|              mean|             day-1|             day-2|             day-3|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "| 8.054166666666667|              null|              null|              null|\n",
      "| 8.054166666666667| 8.054166666666667|              null|              null|\n",
      "|7.9458333333333355| 8.054166666666667| 8.054166666666667|              null|\n",
      "|7.9458333333333355|7.9458333333333355| 8.054166666666667| 8.054166666666667|\n",
      "|10.991666666666665|7.9458333333333355|7.9458333333333355| 8.054166666666667|\n",
      "|10.991666666666665|10.991666666666665|7.9458333333333355|7.9458333333333355|\n",
      "|11.916666666666668|10.991666666666665|10.991666666666665|7.9458333333333355|\n",
      "|11.916666666666668|11.916666666666668|10.991666666666665|10.991666666666665|\n",
      "| 9.679166666666665|11.916666666666668|11.916666666666668|10.991666666666665|\n",
      "| 9.679166666666665| 9.679166666666665|11.916666666666668|11.916666666666668|\n",
      "|             8.375| 9.679166666666665| 9.679166666666665|11.916666666666668|\n",
      "|             8.375|             8.375| 9.679166666666665| 9.679166666666665|\n",
      "| 8.470833333333333|             8.375|             8.375| 9.679166666666665|\n",
      "| 8.470833333333333| 8.470833333333333|             8.375|             8.375|\n",
      "| 8.083333333333334| 8.470833333333333| 8.470833333333333|             8.375|\n",
      "| 9.462499999999999| 8.083333333333334| 8.470833333333333| 8.470833333333333|\n",
      "|            9.1125| 9.462499999999999| 8.083333333333334| 8.470833333333333|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|day-1|day-2|day-3|\n",
      "+-----+-----+-----+\n",
      "|  7.9|  8.1|  8.1|\n",
      "|  7.9|  7.9|  8.1|\n",
      "| 11.0|  7.9|  7.9|\n",
      "| 11.0| 11.0|  7.9|\n",
      "| 11.9| 11.0| 11.0|\n",
      "| 11.9| 11.9| 11.0|\n",
      "|  9.7| 11.9| 11.9|\n",
      "|  9.7|  9.7| 11.9|\n",
      "|  8.4|  9.7|  9.7|\n",
      "|  8.4|  8.4|  9.7|\n",
      "|  8.5|  8.4|  8.4|\n",
      "|  8.5|  8.5|  8.4|\n",
      "|  8.1|  8.5|  8.5|\n",
      "|  9.5|  8.1|  8.5|\n",
      "+-----+-----+-----+\n",
      "\n",
      "+-----------------+\n",
      "|       prediction|\n",
      "+-----------------+\n",
      "|9.046158745660172|\n",
      "|9.950431851293004|\n",
      "|9.512880348567439|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.round\n",
    "import org.apache.spark.sql.functions.lag\n",
    "import org.apache.spark.sql.functions.col \n",
    "import play.api.libs.json._\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.feature.{VectorAssembler}\n",
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "System.out.println(weatherStationsArray.length)\n",
    "// for (weatherStation <- weatherStationsArray) {\n",
    "\n",
    "    val weatherStationID = weatherStations(0)\n",
    "    System.out.println(s\"\"\"Weather Station ID is ${weatherStationID}\"\"\")\n",
    "    \n",
    "    val w = org.apache.spark.sql.expressions.Window.orderBy(\"year\", \"month\", \"day\")  \n",
    "    val dfTrain = dfDailyTemp.withColumn(\"day-1\", lag(col(\"mean\"), 1, null).over(w)).\n",
    "        withColumn(\"day-2\", lag(col(\"mean\"), 2, null).over(w)).\n",
    "        withColumn(\"day-3\", lag(col(\"mean\"), 3, null).over(w))\n",
    "\n",
    "    dfTrain.select(\"mean\", \"day-1\", \"day-2\", \"day-3\").show()\n",
    "\n",
    "    val dfTrain2 = dfTrain.withColumn(\"day-1\", round(col(\"day-1\"), 1)).\n",
    "        withColumn(\"day-2\", round(col(\"day-2\"), 1)).\n",
    "        withColumn(\"day-3\", round(col(\"day-3\"), 1))\n",
    "\n",
    "    val dfTrain3 = dfTrain2.na.drop()\n",
    "\n",
    "    dfTrain3.select(\"day-1\", \"day-2\", \"day-3\").show()\n",
    "    \n",
    "    val splits = dfTrain3.randomSplit(Array(0.8, 0.20), seed = 24L)\n",
    "    val training_data = splits(0)\n",
    "    val test_data = splits(1)\n",
    "    \n",
    "    val features_assembler = new VectorAssembler().\n",
    "        setInputCols(Array(\"day-1\", \"day-2\", \"day-3\")).\n",
    "        setOutputCol(\"features\")\n",
    "    \n",
    "    val linearRegression = new LinearRegression().\n",
    "        setMaxIter(10).\n",
    "        setRegParam(0.3).\n",
    "        setElasticNetParam(0.8).\n",
    "        setLabelCol(\"mean\").\n",
    "        setFeaturesCol(\"features\")\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(features_assembler, linearRegression))\n",
    "\n",
    "    val linearRegressionModel = pipeline.fit(training_data)\n",
    "\n",
    "    val predictions = linearRegressionModel.transform(test_data)\n",
    "    predictions.select(\"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://repo1.maven.org/maven2/org/jpmml/pmml-model/1.4.6/pmml-model-1.4.6.jar\n",
      "Finished download of pmml-model-1.4.6.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://repo1.maven.org/maven2/org/jpmml/pmml-model/1.4.6/pmml-model-1.4.6.jar -f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://central.maven.org/maven2/org/jpmml/jpmml-sparkml/1.4.6/jpmml-sparkml-1.4.6.jar\n",
      "Finished download of jpmml-sparkml-1.4.6.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://central.maven.org/maven2/org/jpmml/jpmml-sparkml/1.4.6/jpmml-sparkml-1.4.6.jar -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:87: error: value pmml is not a member of object org.dmg.pmml.PMML\n",
       "val $ires8 = PMML.pmml\n",
       "                  ^\n",
       "<console>:85: error: value pmml is not a member of object org.dmg.pmml.PMML\n",
       "       PMML pmml = new PMMLBuilder(training_data.schema(), linearRegressionModel).build();\n",
       "            ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.dmg.pmml.PMML\n",
    "import org.jpmml.sparkml.PMMLBuilder\n",
    "\n",
    "PMML pmml = new PMMLBuilder(training_data.schema(), linearRegressionModel).build();\n",
    "\n",
    "// Viewing the result\n",
    "JAXBUtil.marshalPMML(pmml, new StreamResult(System.out));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2018. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
