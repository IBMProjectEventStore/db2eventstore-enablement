{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id=\"open-existing-db\"></a>\n",
    "###  Import the correct libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//import libraries\n",
    "import org.apache.spark.{SparkConf, SparkContext, SparkFiles}\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession, Row}\n",
    "import org.apache.spark.SparkFiles\n",
    "\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.ibm.transformers.RenameColumn\n",
    "\n",
    "import com.ibm.analytics.ngp.repository._\n",
    "import com.ibm.analytics.ngp.ingest.Sampling\n",
    "import com.ibm.analytics.ngp.util._\n",
    "import com.ibm.analytics.ngp.pipeline.evaluate.{Evaluator,MLProblemType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"open-existing-db\"></a>\n",
    "###  Open the IBM Db2 Event store database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import com.ibm.event.oltp.EventContext\n",
    "val eContext = EventContext.getEventContext(\"KillrWeather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validate-db\"></a>\n",
    "###  Validate that the table have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val raw_weather_data = eContext.getTable(\"raw_weather_data\")\n",
    "val sky_condition_lookup = eContext.getTable(\"sky_condition_lookup\")\n",
    "val monthly_aggregate_precip = eContext.getTable(\"monthly_aggregate_precip\")\n",
    "val monthly_aggregate_windspeed = eContext.getTable(\"monthly_aggregate_windspeed\")\n",
    "val monthly_aggregate_pressure = eContext.getTable(\"monthly_aggregate_pressure\")\n",
    "val monthly_aggregate_temperature = eContext.getTable(\"monthly_aggregate_temperature\")\n",
    "val daily_aggregate_precip = eContext.getTable(\"daily_aggregate_precip\")\n",
    "val daily_aggregate_windspeed = eContext.getTable(\"daily_aggregate_windspeed\")\n",
    "val daily_aggregate_pressure = eContext.getTable(\"daily_aggregate_pressure\")\n",
    "val daily_aggregate_temperature = eContext.getTable(\"daily_aggregate_temperature\")\n",
    "val daily_predicted_temperature = eContext.getTable(\"daily_predicted_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-sqlContext\"></a>\n",
    "### Create the IBM Db2 EventSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "import com.ibm.event.oltp.EventContext\n",
    "import org.apache.log4j.{Level, LogManager, Logger}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.ibm.event.EventSession\n",
    "\n",
    "val sqlContext = new EventSession(spark.sparkContext, \"KillrWeather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-DataFrame\"></a>\n",
    "### Prepare a DataFrame for the query \n",
    "The following API provides a DataFrame that holds the query results on the IBM Db2 Event Store table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dfDailyTemp = sqlContext.loadEventTable(\"daily_aggregate_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- wsid: string (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- ts: long (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- mean: double (nullable = false)\n",
      " |-- variance: double (nullable = false)\n",
      " |-- stdev: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDailyTemp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+---+-------------+----+---+------------------+------------------+------------------+\n",
      "|        wsid|year|month|day|           ts|high|low|              mean|          variance|             stdev|\n",
      "+------------+----+-----+---+-------------+----+---+------------------+------------------+------------------+\n",
      "|725030:14732|2011|    9|  1|1317452400183| 3.9|0.0|2.1541666666666663|1.0841493055555556| 1.041224906326945|\n",
      "|725030:14732|2011|    9|  2|1317538800183|12.2|0.0| 6.216666666666667|  6.12888888888889| 2.475659283683619|\n",
      "|725030:14732|2011|    9|  3|1317625200895|10.0|0.0|            4.4375| 8.139010416666666|2.8528950938768616|\n",
      "|725030:14732|2011|    9|  4|1317711600896| 7.8|0.0| 3.858333333333333| 4.009097222222221|2.0022730139074993|\n",
      "|725030:14732|2011|    9|  5|1317798000659|11.1|0.0| 6.929166666666666|10.974565972222223|3.3127882474167016|\n",
      "+------------+----+-----+---+-------------+----+---+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val weatherStations = dfDailyTemp.select(\"wsid\").distinct.collect.flatMap(_.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._\n",
    "val weatherStationsArray = weatherStations.map(ws => dfDailyTemp.where($\"wsid\" <=> ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Weather Station ID is 725030:14732\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|              mean|             day-1|             day-2|             day-3|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|          -1.84375|              null|              null|              null|\n",
      "|             2.925|          -1.84375|              null|              null|\n",
      "| 4.887499999999999|             2.925|          -1.84375|              null|\n",
      "| 9.508333333333333| 4.887499999999999|             2.925|          -1.84375|\n",
      "|13.583333333333332| 9.508333333333333| 4.887499999999999|             2.925|\n",
      "|12.112499999999999|13.583333333333332| 9.508333333333333| 4.887499999999999|\n",
      "| 8.254166666666666|12.112499999999999|13.583333333333332| 9.508333333333333|\n",
      "| 8.504166666666666| 8.254166666666666|12.112499999999999|13.583333333333332|\n",
      "| 8.283333333333331| 8.504166666666666| 8.254166666666666|12.112499999999999|\n",
      "| 5.195833333333334| 8.283333333333331| 8.504166666666666| 8.254166666666666|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|day-1|day-2|day-3|\n",
      "+-----+-----+-----+\n",
      "|  4.9|  2.9| -1.8|\n",
      "|  9.5|  4.9|  2.9|\n",
      "| 13.6|  9.5|  4.9|\n",
      "| 12.1| 13.6|  9.5|\n",
      "|  8.3| 12.1| 13.6|\n",
      "|  8.5|  8.3| 12.1|\n",
      "|  8.3|  8.5|  8.3|\n",
      "+-----+-----+-----+\n",
      "\n",
      "log4j: reset attribute= \"false\".\n",
      "log4j: Threshold =\"null\".\n",
      "log4j: Retreiving an instance of org.apache.log4j.Logger.\n",
      "log4j: Setting [org.apache.zookeeper] additivity to [true].\n",
      "log4j: Level value for org.apache.zookeeper is  [off].\n",
      "log4j: org.apache.zookeeper level set to OFF\n",
      "log4j: Retreiving an instance of org.apache.log4j.Logger.\n",
      "log4j: Setting [org.apache.spark] additivity to [true].\n",
      "log4j: Level value for org.apache.spark is  [off].\n",
      "log4j: org.apache.spark level set to OFF\n",
      "log4j: Retreiving an instance of org.apache.log4j.Logger.\n",
      "log4j: Setting [com.ibm.event] additivity to [true].\n",
      "log4j: Level value for com.ibm.event is  [warn].\n",
      "log4j: com.ibm.event level set to WARN\n",
      "log4j: Retreiving an instance of org.apache.log4j.Logger.\n",
      "log4j: Setting [com.ibm.event.rest] additivity to [true].\n",
      "log4j: Level value for com.ibm.event.rest is  [debug].\n",
      "log4j: com.ibm.event.rest level set to DEBUG\n",
      "log4j: Level value for root is  [off].\n",
      "log4j: root level set to OFF\n",
      "log4j: Class name: [org.apache.log4j.ConsoleAppender]\n",
      "log4j: Setting property [target] to [System.out].\n",
      "log4j: Parsing layout of class: \"org.apache.log4j.PatternLayout\"\n",
      "log4j: Setting property [conversionPattern] to [%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n].\n",
      "log4j: Adding appender named [info-out] to category [root].\n",
      "log4j: Class name: [org.apache.log4j.ConsoleAppender]\n",
      "log4j: Setting property [target] to [System.err].\n",
      "log4j: Parsing layout of class: \"org.apache.log4j.PatternLayout\"\n",
      "log4j: Setting property [conversionPattern] to [%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n].\n",
      "log4j: Setting property [levelMin] to [WARN].\n",
      "log4j: Setting property [levelMax] to [FATAL].\n",
      "log4j: Setting property [acceptOnMatch] to [true].\n",
      "log4j: Adding filter of type [class org.apache.log4j.varia.LevelRangeFilter] to appender named [error-out].\n",
      "log4j: Adding filter of type [class org.apache.log4j.varia.DenyAllFilter] to appender named [error-out].\n",
      "log4j: Adding appender named [error-out] to category [root].\n",
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|25.365987614017186|\n",
      "| 7.944753599002471|\n",
      "+------------------+\n",
      "\n",
      "{\"wsid\":\"725030:14732\",\"pmml\":\"<?xml version='1.0' ?><PMML version='4.3' xmlns='http://www.dmg.org/PMML-4_3' xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation='http://www.dmg.org/PMML-4_3 http://www.dmg.org/v4-3/pmml-4-3.xsd'><Header copyright='(c) Copyright IBM Corp. 2011, 2015' description='linear engine'><Application name='Analytic Framework' version='3.0'></Application><Timestamp>Fri Mar 9 24:35:2 2018</Timestamp></Header><DataDictionary numberOfFields='13'><DataField name='wsid' displayName='wsid' optype='categorical' dataType='string'><Value value='725030:14732' property='valid'></Value></DataField><DataField name='year' displayName='year' optype='continuous' dataType='integer'></DataField><DataField name='month' displayName='month' optype='continuous' dataType='integer'></DataField><DataField name='day' displayName='day' optype='continuous' dataType='integer'></DataField><DataField name='ts' displayName='ts' optype='continuous' dataType='integer'></DataField><DataField name='high' displayName='high' optype='continuous' dataType='double'></DataField><DataField name='low' displayName='low' optype='continuous' dataType='double'></DataField><DataField name='mean' displayName='mean' optype='continuous' dataType='double'></DataField><DataField name='variance' displayName='variance' optype='continuous' dataType='double'></DataField><DataField name='stdev' displayName='stdev' optype='continuous' dataType='double'></DataField><DataField name='day-1' displayName='day-1' optype='continuous' dataType='double'></DataField><DataField name='day-2' displayName='day-2' optype='continuous' dataType='double'></DataField><DataField name='day-3' displayName='day-3' optype='continuous' dataType='double'></DataField></DataDictionary><GeneralRegressionModel modelType='generalLinear' targetVariableName='mean' algorithmName='LE' functionName='regression'><Extension extender='spss.com' name='modelID' value='0'></Extension><MiningSchema><MiningField name='day-1'></MiningField><MiningField name='day-3'></MiningField><MiningField name='mean' usageType='predicted'></MiningField></MiningSchema><ModelStats><UnivariateStats field='mean'><Anova><AnovaRow degreesOfFreedom='3.0' fValue='8.208772313547689' meanOfSquares='14.25134919696064' pValue='0.24987130481327868' sumOfSquares='42.75404759088192' type='Model'></AnovaRow><AnovaRow degreesOfFreedom='1.0' meanOfSquares='1.736112131340314' sumOfSquares='1.736112131340314' type='Error'></AnovaRow><AnovaRow degreesOfFreedom='4.0' sumOfSquares='44.49015972222223' type='Total'></AnovaRow></Anova></UnivariateStats><UnivariateStats field='mean'><Counts totalFreq='5.0'></Counts><NumericInfo maximum='13.583333333333332' mean='9.535833333333333' minimum='5.195833333333334' standardDeviation='3.3350472156411155'></NumericInfo></UnivariateStats><UnivariateStats field='day-1'><Counts totalFreq='5.0'></Counts><NumericInfo maximum='13.6' mean='9.64' minimum='8.3' standardDeviation='2.2689204481426843'></NumericInfo></UnivariateStats><UnivariateStats field='day-3'><Counts totalFreq='5.0'></Counts><NumericInfo maximum='13.6' mean='8.36' minimum='2.9' standardDeviation='4.561578674099571'></NumericInfo></UnivariateStats></ModelStats><Targets><Target field='mean' optype='continuous'></Target></Targets><ParameterList><Parameter label='Intercept' name='P0000001'></Parameter><Parameter label='day-1' name='P0000002'></Parameter><Parameter label='day-3' name='P0000003'></Parameter><Parameter label='day-3 * day-3' name='P0000004'></Parameter></ParameterList><CovariateList><Predictor name='day-1'></Predictor><Predictor name='day-3'></Predictor></CovariateList><PPMatrix><PPCell parameterName='P0000002' predictorName='day-1' value='1'></PPCell><PPCell parameterName='P0000003' predictorName='day-3' value='1'></PPCell><PPCell parameterName='P0000004' predictorName='day-3' value='2'></PPCell></PPMatrix><ParamMatrix><PCell beta='15.634178785570235' df='1' parameterName='P0000001'></PCell><PCell beta='0.6317837771848596' df='1' parameterName='P0000002'></PCell><PCell beta='-3.3565562009366956' df='1' parameterName='P0000003'></PCell><PCell beta='0.18341578967749636' df='1' parameterName='P0000004'></PCell></ParamMatrix></GeneralRegressionModel></PMML>\"}\n",
      "HttpResponse(OK,200,Map(Content-Length -> Vector(2), Content-Type -> Vector(text/plain; charset=UTF-8), Date -> Vector(Fri, 09 Mar 2018 00:35:05 GMT), Server -> Vector(akka-http/10.0.10), Status -> Vector(HTTP/1.1 200 OK), X-RBT-Optimized-By -> Vector(svl-wx-sh2 (RiOS 9.1.3) SC)))"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.round\n",
    "import org.apache.spark.sql.functions.lag\n",
    "import org.apache.spark.sql.functions.col \n",
    "import play.api.libs.json._\n",
    "import scalaj.http.{Http, HttpOptions}\n",
    "import com.ibm.spss.ml.classificationandregression.LinearRegression\n",
    "\n",
    "System.out.println(weatherStationsArray.length)\n",
    "for (weatherStation <- weatherStationsArray) {\n",
    "\n",
    "    val weatherStationID = weatherStation.first()(0)\n",
    "    System.out.println(s\"\"\"Weather Station ID is ${weatherStationID}\"\"\")\n",
    "    \n",
    "    val w = org.apache.spark.sql.expressions.Window.orderBy(\"year\", \"month\", \"day\")  \n",
    "    val dfTrain = dfDailyTemp.withColumn(\"day-1\", lag(col(\"mean\"), 1, null).over(w)).\n",
    "        withColumn(\"day-2\", lag(col(\"mean\"), 2, null).over(w)).\n",
    "        withColumn(\"day-3\", lag(col(\"mean\"), 3, null).over(w))\n",
    "\n",
    "    dfTrain.select(\"mean\", \"day-1\", \"day-2\", \"day-3\").show()\n",
    "\n",
    "    val dfTrain2 = dfTrain.withColumn(\"day-1\", round(col(\"day-1\"), 1)).\n",
    "        withColumn(\"day-2\", round(col(\"day-2\"), 1)).\n",
    "        withColumn(\"day-3\", round(col(\"day-3\"), 1))\n",
    "\n",
    "    val dfTrain3 = dfTrain2.na.drop()\n",
    "\n",
    "    dfTrain3.select(\"day-1\", \"day-2\", \"day-3\").show()\n",
    "    \n",
    "    val splits = dfTrain3.randomSplit(Array(0.8, 0.20), seed = 24L)\n",
    "    val training_data = splits(0)\n",
    "    val test_data = splits(1)\n",
    "\n",
    "    val linearRegression = LinearRegression().\n",
    "        setInputFieldList(Array(\"day-1\", \"day-2\", \"day-3\")).\n",
    "        setTargetField(\"mean\")\n",
    "\n",
    "    val linearRegressionModel = linearRegression.fit(training_data)\n",
    "\n",
    "    val predictions = linearRegressionModel.transform(test_data)\n",
    "    predictions.select(\"prediction\").show()\n",
    "\n",
    "    val pmml = linearRegressionModel.toPMML().replace('\\\"', '\\'')\n",
    "\n",
    "    val online_path = \"http://think-demo.lightbend.com/model\"\n",
    "    val modelString = s\"\"\"{\"wsid\":\"${weatherStationID}\",\"pmml\":\"${pmml.toString}\"}\"\"\"\n",
    "    System.out.println (modelString)\n",
    "\n",
    "    val response_online = Http(online_path).postData(modelString).header(\"Content-Type\", \"application/json\").option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\n",
    "    print (response_online)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2018. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
